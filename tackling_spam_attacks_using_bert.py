# -*- coding: utf-8 -*-
"""Tackling spam attacks using BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_pzxQ4ssG8i4uCHc6RqRNTPUQqIwtECg

# Tackling mad-lib attacks in spam classification using BERT 

v1.0, Jun-2021 (c) Sergio A. Rojas-Galeano. 
Email: srojas@udistrital.edu.co

*NB: This NoteBook is released under the terms of the GNU General Public License (see license at: https://www.gnu.org/licenses/gpl-3.0.txt).
This software is made publicly available in the hope that it will be useful
to modelers, but WITHOUT ANY WARRANTY whatsoever.*

---

This a companion NoteBook to the paper "Tackling Mad-lib Spam Attacks using BERT Encoding". In that paper we investigated whether the BERT language model, may be useful to resist the synonym substitution adversarial attack (called “Mad-lib” as per the word substitution game) against spam classifiers. For this purpose we compared different document representation models (BoW, TFIDF and BERT) coupled with several classifiers (Decision Tree, kNN, SVM, Logistic Regression, Naive Bayes, Multilayer Perceptron), on th SMS spam collection dataset from the UCI repository.

# Initialisation and data loading

These are preliminary steps to setup the experimental testbed.
"""

## Install and import plotting libraries ##
!pip install scikit-plot
import scikitplot.plotters as skplt

## Install and import thesaurus libraries ##
!pip install PyDictionary
from PyDictionary import PyDictionary

## Install and import BERT libraries ##
!pip install -U sentence-transformers
from sentence_transformers import SentenceTransformer, util

print("\n\n Installation done!")

## Import pre-installed libraries ##
import warnings
warnings.filterwarnings("ignore")

import pandas as pd
pd.set_option('display.max_colwidth', None)

import io
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, balanced_accuracy_score
from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, MinMaxScaler

import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk import word_tokenize
from nltk.corpus import stopwords
from nltk.data import load
from nltk.stem import SnowballStemmer
from string import punctuation

from pprint import pprint; 
import pickle

print("\n\n Libraries loaded!")

## Open data connection to Drive ##
from google.colab import drive 
drive.mount('/content/gdrive')

## Load file into data frame ##
df = pd.read_csv('gdrive/My Drive/Colab Notebooks/data/spam/spam.csv', skiprows=1, names=['label', 'message'], encoding="ISO-8859-1")

## Display some statistics: histogram of text labels ##
_, ax = plt.subplots(1,2)
df['label'].value_counts().plot(ax=ax[0], kind="bar", rot=90, title='label');
df['label'].value_counts().plot(ax=ax[1], kind="pie", rot=90, title='label', ylabel='');
print("Dataset size: ", df.shape)

df.head(10)

"""
# NLP setup

Here we defined functions for text pre-processing and cleanup."""

## Create some ancilliary functions to preprocess spanish words ##
stemmer = SnowballStemmer('english')

def stem_tokens(tokens, stemmer):
    stems = []
    for token in tokens:
        stems.append(stemmer.stem(token))
    return stems

def tokenize(text):
    text = ''.join([s for s in text if s not in non_words])
    tokens = word_tokenize(text)
    return tokens

def tokenize_stem(text):
    text = ''.join([s for s in text if s not in non_words])
    tokens = word_tokenize(text)
    tokens = stem_tokens(tokens, stemmer)
    return tokens

## Prepare non-words (punctuation) list ##
non_words = list(punctuation)
non_words.extend(map(str,range(10)))
print("Symbols to be removed (%d):" % len(non_words), non_words)

## Prepare english stopwords list ##
english_stopwords = stopwords.words('english')
print("Stop-words to be removed (%d):" % len(english_stopwords), english_stopwords)

"""# Build or load thesaurus

In order to perform the Mad-lib attacks automatically, we need a thesaurus. Thus, we extracted a vocabulary of the 5000 most frequent terms from the entire dataset and used them as keywords in a thesaurus. For each keyword, a list of synonyms is automatically scrapped from the pages of the website www.dictionary.com.

Notice that some text sequences (either proper words or gibberish) may not have synonyms found on the website. 
"""

## Setup a BoW instance and thesaurus ##
bow = CountVectorizer(
                analyzer = 'word',
                # ngram_range = (1, 2),
                strip_accents = 'ascii',
                tokenizer = tokenize,
                lowercase = True,
                stop_words = english_stopwords,
                max_features = 10000,     # A large limit allows BoW to extract the entire vocabulary
                )

dictionary = PyDictionary()               # This is the web scrapping object

## Get full vocabulary list and obtain their synonyms by scrapping https://www.synonym.com ##
print("Building thesaurus...")
bow.fit_transform(df['message'])
vocab = bow.get_feature_names()   
# print(len(vocab))
# pprint(vocab)

thesaurus = defaultdict(set)      

for i, word in enumerate(vocab):
  synonyms = dictionary.synonym(word)
  if synonyms is not None:
    thesaurus[word] = synonyms
    print("%s ->" % word, thesaurus[word])

## Save to a file ##
file = 'gdrive/My Drive/Colab Notebooks/data/spam/synonyms-5000.pkl' 
f = open(file,"wb")
pickle.dump(thesaurus, f)
f.close()

print("Thesaurus done!")

len(thesaurus)

## Have a peek at the thesaurus ##
file = 'gdrive/My Drive/Colab Notebooks/data/spam/synonyms-5000.pkl' 
synonyms = pickle.load(open(file, "rb"))
pprint(synonyms)

"""# Setup Bag of Words (BoW) encoders

Here we setup and try the BoW and TFIDF encoders from the sckit-learn library.

"""

## Create BoW encoders ##
bow = CountVectorizer(
                analyzer = 'word',
                # ngram_range = (1, 2),
                strip_accents = 'ascii',
                tokenizer = tokenize_stem,
                lowercase = True,
                stop_words = english_stopwords,
                max_features = 768,         # We set the same dim as BERT
                )
tfidf = TfidfVectorizer(
                analyzer = 'word',
                # ngram_range = (2, 2),
                strip_accents = 'ascii',
                tokenizer = tokenize_stem,
                lowercase = True,
                stop_words = english_stopwords,
                max_features = 768,
                )

print("BoW setup done!")

## Quick try of these encoders on a subset of messages ##
N_DOCS = 1000
bow_vecs = bow.fit_transform(df['message'][:N_DOCS])
tfidf_vecs = tfidf.fit_transform(df['message'][:N_DOCS])

df_bow = pd.DataFrame(data=bow_vecs.todense(), columns=bow.get_feature_names())
df_tfidf = pd.DataFrame(data=tfidf_vecs.todense(), columns=tfidf.get_feature_names())
display(df_bow.head(), df_tfidf.head())
print("BoW data size: ", bow_vecs.shape)
print("TFiDF data size: ", tfidf_vecs.shape)
print("Vocabulary: ", bow.get_feature_names())

"""# Setup BERT encoders

Here we setup and try the BERT encoder from the sentence_transformers library.
"""

## Create BERT encoder ##
model = SentenceTransformer('xlm-r-bert-base-nli-stsb-mean-tokens')
print("\n\nBERT pre-trained model loaded!")

## Ancilliary function to perform BERT encoding of a set of docs ##
def bert_transform(docs, encoder):    
  print("\n\nEncoding docs using BERT model: ")
  doc_vecs = []   
  for i, doc in enumerate(docs):
    doc_vec = encoder.encode(doc)
    doc_vecs.append(doc_vec)      
    print(".", end="" if (i%100>0) else ("%4d\n"%i))

  print("\nDocs embeddings ready!")
  return doc_vecs

print("BERT encoding function ready!")

## Ancilliary function to visualise the document encodings ##
def show_embeddings(doc_vecs):
  nreps = len(doc_vecs)
  fig, axs = plt.subplots(nrows=nreps, sharex=True, figsize = (16, 3*nreps))

  for i, key in enumerate(doc_vecs.keys()):
    enc_name = ("Encoding: %s") % key
    sns.heatmap(1-doc_vecs[key], cbar=False, ax=axs[i]).set_title(enc_name);

bert_vecs = bert_transform(df['message'][:N_DOCS], model)
  spam_vecs = defaultdict(list)
  spam_vecs["BoW"] = bow_vecs.todense().A
  spam_vecs["TFiDF"] = tfidf_vecs.todense().A
  spam_vecs["BERT"] = np.array(bert_vecs)
  
  # Display the vector representation of each message by the three encoders #
  # BoW and TFIDF yield sparse representations, whereas BERT yield dense    # 
  show_embeddings(spam_vecs)

"""# Setup classifiers

Now we set up the classification algorithms with the best set of parameters found in previous experiments. We also define train and test functions.
"""

classifiers = [
    KNeighborsClassifier(15),
    LogisticRegression(),
    LinearSVC(C=1, loss='squared_hinge', max_iter=1000, random_state=None, penalty='l2', tol=0.0001),
    SVC(gamma=.01, C=100),
    MLPClassifier(hidden_layer_sizes=(10,), alpha=1, max_iter=1000),
    DecisionTreeClassifier(max_depth=10),
    GaussianNB(),
    ]

print("Classifiers ready!")

## Ancilliary function to train and test a classifier ##
def train_test_clf(clf, X_train, y_train, X_test, y_test, tag="", viz=False):
  """
  Args:
      clf:     Classifier object
      X_train: Encoding vectors of training docs   
      y_train: Labels of training docs
      X_test:  Encoding vectors of test docs   
      y_test:  Labels of test docs        
  """
  ## Perform classification ##
  clf.fit(X_train, y_train)
  y_pred = clf.predict(X_test)

  ## Compute classification metrics ##
  acc = accuracy_score(y_test, y_pred)
  bacc = balanced_accuracy_score(y_test, y_pred)
  prec, sens, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label='spam')

  ## Show results ##
  print('{:<36s} Balanced accuracy: {:.2f}%'.format(tag, bacc*100))
  if viz:
    skplt.plot_confusion_matrix(y_test, y_pred, title=tag+'BAcc: {0:.2f}%'.format(bacc*100), 
                              normalize=False, x_tick_rotation='vertical');
    plt.show()

  return acc, bacc, prec, sens, f1
      
print("Train/test function ready!")

## Ancilliary function to test a classifier ##
def test_clf(clf, X_test, y_test, tag="", viz=False):
  """
  Args:
      clf:     Pre-trained classifier object 
      X_test:  Encoding vectors of test docs   
      y_test:  Labels of test docs        
  """
  ## Perform classification ##
  y_pred = clf.predict(X_test)

  ## Compute classification metrics ##
  acc = accuracy_score(y_test, y_pred)
  bacc = balanced_accuracy_score(y_test, y_pred)
  prec, sens, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label='spam')

  ## Show results ##
  print('{:<36s} Balanced accuracy: {:.2f}%'.format(tag, bacc*100))
  if viz:
    skplt.plot_confusion_matrix(y_test, y_pred, title=tag+'BAcc: {0:.2f}%'.format(bacc*100), 
                              normalize=False, x_tick_rotation='vertical');
    plt.show()

  return acc, bacc, prec, sens, f1

print("Test function ready!")

"""# Down-stream spam classification experiments

The code in this section is used to conduct two experiments: 
* Spam classification of original messages
* Spam classification of modified messages with Mad-lib attack
"""

## Ancilliary function to split the dataset into development (80%) and holdout (20%) ##
def split_dataset(df, n_docs=100, viz=False):
  D_dev, D_hold, y_dev, y_hold = train_test_split(df['message'][:n_docs], 
                                                  df['label'][:n_docs], 
                                                  test_size=0.2, 
                                                  shuffle=True)
  ## Display data statistics ##
  if viz:
    _, ax = plt.subplots(1, 2)
    y_dev.value_counts().plot(ax=ax[0], kind='pie', figsize=(8, 8), 
                               autopct=lambda x: "%d" % round(y_dev.shape[0]*x/100), 
                               ylabel='', title='Development')
    y_hold.value_counts().plot(ax=ax[1], kind='pie', figsize=(8, 8), 
                               autopct=lambda x: "%d" % round(y_hold.shape[0]*x/100), 
                               ylabel='', title='Hold-out')
    plt.show()
    display("Dev split contents:", D_dev.to_frame().head(5))
    display("Hold-out split contents:", D_hold.to_frame().head(5))
    print("Dev split size (docs/labels): ", D_dev.shape, y_dev.shape)
    print("Hold-out split size (docs/labels): ", D_hold.shape, y_hold.shape)

  return D_dev, D_hold, y_dev, y_hold

print("Split function ready!")

## Try some splits #
split_dataset(df, viz=True);
split_dataset(df, viz=True);

import re

## Set number of documents to process ##
# N_DOCS = 1000
N_DOCS = df.shape[0]
N_REPS = 30
N_SUBS = 10
print("Number of documents to process: ", N_DOCS)
print("Number of replicates: ", N_REPS)

## Load thesaurus ##
thesfile = 'gdrive/My Drive/Colab Notebooks/data/spam/synonyms-5000.pkl' 
thesaurus = pickle.load(open(thesfile, "rb"))

## Initialize results holders ##
np.random.seed(seed=20212021)     # Seed is fixed to ensure reproducibility

results1 = pd.DataFrame(columns=['Rep', 'Encoder', 'Classifier', 'Acc', 
                                 'BAcc', 'Prec', 'Sens', 'F1'])
results2 = pd.DataFrame(columns=['Rep', 'Subs', 'True_subs', 'Encoder', 
                                 'Classifier', 'Acc', 
                                 'BAcc', 'Prec', 'Sens', 'F1'])

for i_rep in range(N_REPS):

  ###########################################################################
  ############## Spam classification experiments (D_dev split) ##############
  ###########################################################################
  print("\n\n------ Replicate: %d ------" % (i_rep+1))
  D_dev, D_hold, y_dev, y_hold = split_dataset(df, N_DOCS, viz=False)

  ## Obtain BoW spam message encodings ##
  bow_vecs = bow.fit_transform(D_dev)
  tfidf_vecs = tfidf.fit_transform(D_dev)
  print("\n\nComputing BoW encoding... done!")

  ## Obtain BERT spam message encodings ##
  bert_vecs = bert_transform(D_dev, model)
  print("BERT encoding done!\n")

  spam_vecs = defaultdict(list)
  spam_vecs["BoW"] = bow_vecs.todense().A
  spam_vecs["TFiDF"] = tfidf_vecs.todense().A
  spam_vecs["BERT"] = np.array(bert_vecs)

  ## Try the set of classifiers on each encoding ##
  for key in spam_vecs.keys():
    X_train, X_test, y_train, y_test = train_test_split(spam_vecs[key], y_dev, 
                                                        test_size=0.25)
    for clf in classifiers:
      acc, bacc, prec, sens, f1 = train_test_clf(clf, X_train, y_train, X_test, y_test, 
                             "[%s %s] " % (key, type(clf).__name__), viz=False)
      ## Record results ##
      results1 = results1.append({'Rep': i_rep, 'Encoder': key, 
                                  'Classifier': type(clf).__name__, 
                                  'Acc': acc, 'BAcc': bacc, 
                                  'Prec': prec, 'Sens': sens,
                                  'F1': f1}, ignore_index=True)

  ###########################################################################
  ############# Attack classification experiments (D_hold split) ############
  ###########################################################################
  for i_sub in [0, 5, 10]: #range(N_SUBS):
    print("\n\n------ Rep: %d, Substitutions: %d ------" % (i_rep, i_sub))

    ## Firstly, build the modified (attacked) dataset with substitutions ##
    D_attack = []; subs_count = []
    for doc in D_hold:
      print("\nOriginal: " + doc)
      doc = re.sub('[^a-zA-Z0-9_]', ' ', doc)
      words = doc.lower().split(" ")
      n_words = len(words)
      idx = np.random.choice(n_words, size=min(i_sub, n_words), replace=False)
      print(idx)
      count = 0
      for i in idx:
        synonyms = thesaurus[words[i]]
        if synonyms:
          substitute = np.random.choice(synonyms) 
          print(words[i], "-->", substitute)  
          words[i] = substitute
          count += 1
        else:
          print("Not synonyms found for word: %s" % words[i])
      doc = " ".join(words)
      print("Attacked (%i subs): %s" % (count, doc))
      D_attack.append(doc)
      subs_count.append(count)
      
    ## Now obtain BoW attacked message encodings (with pretrained BoW) ##
    bow_vecs = bow.transform(D_attack)
    tfidf_vecs = tfidf.transform(D_attack)
    print("\n\nComputing BoW encoding... done!")

    ## Now obtain BERT attacked message encodings ##
    bert_vecs = bert_transform(D_attack, model)
    print("BERT encoding done!\n")

    attack_vecs = defaultdict(list)
    attack_vecs["BoW"] = bow_vecs.todense().A
    attack_vecs["TFiDF"] = tfidf_vecs.todense().A
    attack_vecs["BERT"] = np.array(bert_vecs)

    ## Lastly, try the set of classifiers on each encoding ##
    for key in attack_vecs.keys():
      for clf in classifiers:
        acc, bacc, prec, sens, f1 = test_clf(clf, attack_vecs[key], y_hold, 
                              "[%s %s] " % (key, type(clf).__name__), viz=False)
        ## Record results ##
        results2 = results2.append({'Rep': i_rep, 'Subs': i_sub, 
                                    'True_subs': np.mean(subs_count),
                                    'Encoder': key, 
                                    'Classifier': type(clf).__name__, 
                                    'Acc': acc, 'BAcc': bacc, 
                                    'Prec': prec, 'Sens': sens,
                                    'F1': f1}, ignore_index=True)

## Store results ##
pprint(results1)
file = 'gdrive/My Drive/Colab Notebooks/data/spam/clf-results1.pkl' 
f = open(file,"wb")
pickle.dump(results1, f)
f.close()

pprint(results2)
file = 'gdrive/My Drive/Colab Notebooks/data/spam/clf-results2.pkl' 
f = open(file,"wb")
pickle.dump(results2, f)
f.close()

print("Done!")

"""# Results summary"""

## Summarise the results of the original spam classification ##
file = 'gdrive/My Drive/Colab Notebooks/data/spam/clf-results1.pkl' 
results = pickle.load(open(file, "rb"))

pd.options.display.float_format = '{:,.1%}'.format
pd.options.display.max_rows = None

print("\n\n>>>>> Classification experiments: ", results.shape)

# Display general statistics #
display(results.groupby(['Encoder']).describe())

aggregations = {
  'BAcc': [lambda x: "%.1f+-%.1f%%" % (np.mean(x)*100, np.std(x)*100)],
  'Acc' : [lambda x: "%.1f+-%.1f%%" % (np.mean(x)*100, np.std(x)*100)],
  'Sens': [lambda x: "%.1f+-%.1f%%" % (np.mean(x)*100, np.std(x)*100)],
  'Prec': [lambda x: "%.1f+-%.1f%%" % (np.mean(x)*100, np.std(x)*100)],
  # 'Sens': [np.mean, np.std, min],
}

# Display aggregated statistics by encoder and classifier #
display(results.groupby(['Encoder','Classifier']).agg(aggregations))

## Summarise the results of the mad-lib spam attack classification ##

file = 'gdrive/My Drive/Colab Notebooks/data/spam/clf-results2.pkl' 
results = pickle.load(open(file, "rb"))
print("\n\n>>>>> Attack experiments: ", results.shape)

# Display general statistics #
display(results.groupby(['Encoder']).describe())

aggregations = {
  'True_subs': [lambda x: "%.2f" % np.mean(x)],
  'BAcc': [lambda x: "%.1f+-%.1f%%" % (np.mean(x)*100, np.std(x)*100)],
  'Acc' : [lambda x: "%.1f+-%.1f%%" % (np.mean(x)*100, np.std(x)*100)],
  'Sens': [lambda x: "%.1f+-%.1f%%" % (np.mean(x)*100, np.std(x)*100)],
  'Prec': [lambda x: "%.1f+-%.1f%%" % (np.mean(x)*100, np.std(x)*100)],
        # 'F1': [np.mean, np.std, min],
}

# Display aggregated statistics by encoder with SVM Linear classifier fixed #
display(results.query("Classifier == 'LinearSVC'").groupby(['Subs','Encoder']).agg(aggregations))

"""# Conclusion

The results provide empirical evidence suggesting BERT encodings are able to resist the Mad-lib spam attack, whereas BoW and TDIDF detection rates drop dramatically. On the computational side, BERT is heavier than the simpler BoW encoders which nonetheless, are able to achieve comparable performances with spam not tampered with by Mad-lib adversaries. 

---
**END OF NOTEBOOK**

"""